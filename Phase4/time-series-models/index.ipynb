{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Models\n",
    "\n",
    "## TOC\n",
    "\n",
    "- [Random Walk Model](#random_walk)\n",
    "- [Improvements on the FSM](#arma_models)\n",
    "    - [Autoregressive Model](#ar_model)\n",
    "    - [Moving Average Model](#ma_model)\n",
    "    - [ACF and PACF](#acf_pacf)\n",
    "- [auto_arima](#auto_arima)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think back to our lecture on the bias-variance tradeoff, a perfect model is not possible.  There will always be noise (inexplicable error).\n",
    "\n",
    "If we were to remove all of the patterns from our time series, we would be left with white noise, which is written mathematically as:\n",
    "\n",
    "$$\\Large Y_t =  \\epsilon_t$$\n",
    "\n",
    "The error term is randomly distributed around the mean, has constant variance, and no autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some white noise!\n",
    "from random import gauss as gs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rands = []\n",
    "\n",
    "# Create a 1000 cycle for-loop\n",
    "for _ in range(1000):\n",
    "    # Append 1000 random numbers from the standard normal distribution\n",
    "    rands.append(gs(0, 1))\n",
    "    \n",
    "series = pd.Series(rands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Create an array with the same shape as our random gaussian series.\n",
    "X = np.linspace(-10, 10, 1000)\n",
    "\n",
    "ax.plot(X, series)\n",
    "ax.set_title('White Noise Time Series');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know this data has no true pattern governing its fluctuations (because we coded it with a random function).\n",
    "\n",
    "Any attempt at a model would be fruitless.  The next point in the series could be any value, completely independent of the previous value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the timeseries data that we are working with is more than just white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impor chicago gun crime data, and apply some preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv('data/Gun_Crimes_Heat_Map.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# ts_minute = ts.groupby('Date').count()['ID']\n",
    "daily_count = ts.resample('D').count()['ID']\n",
    "\n",
    "# Remove outliers\n",
    "daily_count = daily_count[daily_count < 90]\n",
    "\n",
    "# Create date_range object with all dates between beginning and end dates\n",
    "ts_dr = pd.date_range(daily_count.index[0], daily_count.index[-1])\n",
    "\n",
    "# Create empty series the length of the date range\n",
    "ts_daily = np.empty(shape=len(ts_dr))\n",
    "ts_daily = pd.Series(ts_daily)\n",
    "\n",
    "# Add datetime index to series and fill with daily count\n",
    "ts_daily = ts_daily.reindex(ts_dr)\n",
    "ts_daily = ts_daily.fillna(daily_count)\n",
    "\n",
    "# There are missing values which we fill via linear interpolation\n",
    "ts_daily = ts_daily.interpolate()\n",
    "\n",
    "# Downsample to the week level\n",
    "ts_weekly = ts_daily.resample('W').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ts_weekly)\n",
    "ax.set_title(\"Weekly Reports of Gun Offenses in Chicago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split for a time series is a little different than what we are used to.  Because **chronological order matters**, we cannot randomly sample points in our data.  Instead, we cut off a portion of our data at the end, and reserve it as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index which allows us to split off 20% of the data\n",
    "# Calculate the 80% mark by multiplying the row index of the shape attribute by .8\n",
    "# Use the built in round function to find the nearest integer\n",
    "end_of_train_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test sets according to the index found above\n",
    "train = None\n",
    "test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train)\n",
    "ax.plot(test)\n",
    "ax.set_title('Train test Split');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set aside our test set, and build our model on the train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random_walk'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good first attempt at a model for a time series would be to simply predict the next data point with the point previous to it.  \n",
    "\n",
    "We call this type of time series a random walk, and it is written mathematically like so.\n",
    "\n",
    "$$\\Large Y_t = Y_{t-1} + \\epsilon_t$$\n",
    "\n",
    "$\\epsilon$ represents white noise error.  The formula indicates that the difference between a point and a point before it is white noise.\n",
    "\n",
    "$$\\Large Y_t - Y_{t-1}=  \\epsilon_t$$\n",
    "\n",
    "This makes sense, given one way we described making our series stationary was by applying a difference of a lag of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple random walk model for our Gun Crime dataset.\n",
    "\n",
    "WE can perform this with the shift operator, which shifts our time series according to periods argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction for the next day is the original series shifted to the future by one day.\n",
    "# pass period= 1 argument to the shift method called at the end of train.\n",
    "random_walk = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "train[0:30].plot(ax=ax, c='r', label='original')\n",
    "random_walk[0:30].plot(ax=ax, c='b', label='shifted')\n",
    "ax.set_title('Random Walk')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a random walk as our **FSM**.  \n",
    "\n",
    "That being the case, let's use a familiar metric, RMSE, to assess its strength.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either proceed by hand or using the mean_squared_error function\n",
    "# from sklearn.metrics module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arma_models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement on FSM: Autoregressive and Moving Average Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the residuals from the random walk model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = None\n",
    "\n",
    "plt.plot(residuals.index, residuals)\n",
    "plt.plot(residuals.index, residuals.rolling(30).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the rolling standard deviation of our errors, we can see that the performance of our model varies at different points in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals.index, residuals.rolling(30).var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a result of the trends in our data.\n",
    "\n",
    "Time Series models expect to be fed **stationary** data.  Were able to make our series stationary by differencing our data.\n",
    "\n",
    "Let's repeat that process here. \n",
    "\n",
    "In order to make our life easier, we will use statsmodels to difference our data via the **ARIMA** class. \n",
    "\n",
    "We will break down what ARIMA is shortly, but for now, we will focus on the I, which stands for **integrated**.  A time series which has been be differenced to become stationary is said to have been integrated [1](https://people.duke.edu/~rnau/411arim.htm). \n",
    "\n",
    "There is an order parameter in ARIMA with three slots: (p, d, q).  d represents our order of differencing, so putting a one there in our model will apply a first order difference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an arima_model object, and pass the training set and order (0,1,0) as arguments\n",
    "# then call the .fit() method\n",
    "rw = ARIMA()\n",
    "\n",
    "# Just like our other models, we can now use the predict method. \n",
    "# Add typ='levels' argument to predict on original scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the differenced predictions (d=1) are just a random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put a typ='levels' to convert our predictions to remove the differencing performed.\n",
    "y_hat = rw.predict(typ='levels')\n",
    "\n",
    "# RMSE is equivalent to Random Walk RMSE\n",
    "np.sqrt(mean_squared_error(train[1:], y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing the trend from our data, we assume that our data passes a significance test that the mean and variance are constant throughout.  But it is not just white noise.  If it were, our models could do no better than random predictions around the mean.  \n",
    "\n",
    "Our task now is to find **more patterns** in the series.  \n",
    "\n",
    "We will focus on the data points near to the point in question.  We can attempt to find patterns to how much influence previous points in the sequence have. \n",
    "\n",
    "If that made you think of regression, great! What we will be doing is assigning weights, like our betas, to previous points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ar_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Autoregressive Model (AR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next attempt at a model is the autoregressive model, which is a timeseries regressed on its previous values\n",
    "\n",
    "### $y_{t} = \\phi_{0} + \\phi_{1}y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "The above formula is a first order autoregressive model (AR1), which finds the best fit weight $\\phi$ which, multiplied by the point previous to a point in question, yields the best fit model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our ARIMA model, the **p** variable of the order (p,d,q) represents the AR term.  For a first order AR model, we put a 1 there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an 1st order differenced AR 1 model with the ARIMA class, \n",
    "# Pass train and order (1,1,0)\n",
    "ar_1 = ARIMA(train, (1,1,0)).fit()\n",
    "\n",
    "# predict while passing typ='levels' as an argument\n",
    "ar_1.predict(typ='levels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARIMA class comes with a nice summary table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, as you may notice, the output does not include RMSE.\n",
    "\n",
    "It does include AIC. We briefly touched on AIC with linear regression.  It is a metric with a strict penalty applied to we used models with too many features.  A better model has a lower AIC.\n",
    "\n",
    "Let's compare the first order autoregressive model to our Random Walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_model = ARIMA(train, (0,1,0)).fit()\n",
    "rw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Walk AIC: {rw.aic}')\n",
    "print(f'AR(1,1,0) AIC: {ar_1.aic}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AIC for the AR(1) model is lower than the random walk, indicating improvement.  \n",
    "\n",
    "Let's stick with the RMSE, so we can compare to the hold out data at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ar1 = ar_1.predict(typ='levels')\n",
    "rmse_ar1 = np.sqrt(mean_squared_error(train[1:], y_hat_ar1))\n",
    "rmse_ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_rw = rw.predict(typ='levels')\n",
    "rmse_rw = np.sqrt(mean_squared_error(train[1:], y_hat_rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_rw)\n",
    "print(rmse_ar1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks out. RMSE is lower as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregression, as we said before, is a regression of a time series on lagged values of itself.  \n",
    "\n",
    "From the summary, we see the coefficient of the 1st lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the arparams attribute of the fit ar_1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We come close to reproducing this coefficients with linear regression, with slight differences due to how statsmodels performs the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(np.array(train.diff().shift(1).dropna()).reshape(-1,1), train[1:].diff().dropna())\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also factor in more than just the most recent point.\n",
    "$$\\large y_{t} = \\phi_{0} + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2}+ \\varepsilon_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the order of our AR model by the number of lags back we go.  The above formula refers to an **AR(2)** model.  We put a 2 in the p position of the ARIMA class order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a 1st order difference 2nd order ARIMA model \n",
    "ar_2 = None\n",
    "\n",
    "y_hat_ar_2 = ar_2.predict(typ='levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_ar2 = np.sqrt(mean_squared_error(train[1:], y_hat_ar_2))\n",
    "print(rmse_ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_rw)\n",
    "print(rmse_ar1)\n",
    "print(rmse_ar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ma_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average Model (MA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next type of model is based on error.  The idea behind the moving average model is to make a prediciton based on how far off we were the day before.\n",
    "\n",
    "$$\\large Y_t = \\mu +\\epsilon_t + \\theta * \\epsilon_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moving average model is a pretty cool idea. We make a prediction, see how far off we were, then adjust our next prediction by a factor of how far off our pervious prediction was.\n",
    "\n",
    "In our ARIMA model, the q term of our order (p,d,q) refers to the MA component. To use one lagged error, we put 1 in the q position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_1 = ARIMA(train, (0,0,1)).fit()\n",
    "y_hat = ma_1.predict(typ='levels')\n",
    "y_hat[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 1st order MA model with a 1st order difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_1 = ARIMA(train, <replace_with_correct_order>).fit()\n",
    "rmse_ma1 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(rmse_rw)\n",
    "print(rmse_ar1)\n",
    "print(rmse_ar2)\n",
    "print(rmse_ma1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs better than a 1st order AR, but worse than a 2nd order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like our AR models, we can lag back as far as we want. Our MA(2) model would use the past two lagged terms:\n",
    "\n",
    "$$\\large Y_t = \\mu +\\epsilon_t + \\theta_{t-1} * \\epsilon_{t-1} + \\theta_2 * \\epsilon_{t-2}$$\n",
    "\n",
    "and our MA term would be two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_2 = ARIMA(train, (0,1,2)).fit()\n",
    "y_hat = ma_2.predict(typ='levels')\n",
    "rmse_ma2 = np.sqrt(mean_squared_error(train[1:], y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_rw)\n",
    "print(rmse_ar1)\n",
    "print(rmse_ar2)\n",
    "print(rmse_ma1)\n",
    "print(rmse_ma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to limit ourselves to just AR or MA.  We can use both AR terms and MA terms.\n",
    "\n",
    "for example, an ARMA(2,1) model is given by:\n",
    "\n",
    " $$\\large Y_t = \\mu + \\phi_1 Y_{t-1}+\\phi_2 Y_{t-2}+ \\theta \\epsilon_{t-1}+\\epsilon_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acf_pacf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACF and PACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been able to reduce our AIC by chance, adding fairly random p,d,q terms.\n",
    "\n",
    "We have two tools to help guide us in these decisions: the **autocorrelation** and **partial autocorrelation** functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACF\n",
    "\n",
    "In general, a partial correlation is a **conditional correlation**. It is the  amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags.  \n",
    "\n",
    "If $Y_t$ is correlated with $Y_{t-1}$, and $Y_{t-1}$ is equally correlated with $Y_{t-2}$, then we should also expect to find correlation between $Y_t$ and $Y_{t-2}$.   \n",
    "\n",
    "Thus, the correlation at lag 1 \"propagates\" to lag 2 and presumably to higher-order lags. The partial autocorrelation at lag 2 is therefore the difference between the actual correlation at lag 2 and the expected correlation due to the propagation of correlation at lag 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an AR process, we run a linear regression on lags according to the order of the AR process. The coefficients calculated factor in the influence of the other variables.   \n",
    "\n",
    "Since the PACF shows the direct effect of previous lags, it helps us choose AR terms.  If there is a significant positive value at a lag, consider adding an AR term according to the number that you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules of thumb: \n",
    "\n",
    "    - A sharp drop after lag \"k\" suggests an AR-K model.\n",
    "    - A gradual decline suggests an MA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ar1_pacf](img/ar1_pacf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://online.stat.psu.edu/stat510/book/export/html/665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF\n",
    "\n",
    "The autocorrelation plot of our time series is simply a version of the correlation plots we used in linear regression.  In place of the independent features we include the lags. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train)\n",
    "df.columns = ['lag_0']\n",
    "df['lag_1'] = train.shift()\n",
    "df['lag_2'] = train.shift(2)\n",
    "df['lag_3'] = train.shift(3)\n",
    "df['lag_4'] = train.shift(4)\n",
    "df['lag_5'] = train.shift(5)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.corr()['lag_0'].index)\n",
    "plt.bar(list(df.corr()['lag_0'].index), list(df.corr()['lag_0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error terms in a Moving Average process are built progressively by adjusting the error of the previous moment in time.  Each error term therein includes the indirect effect of the error term before it. Because of this, we can choose the MA term based on how many significant lags appear in the ACF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![acf_ma1](img/ma1_acf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring in the pacf and acf from statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "\n",
    "plot_acf(train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above autocorrelation shows that there is correlation between lags up to about 12 weeks back.  \n",
    "\n",
    "When Looking at the ACF graph for the original data, we see a strong persistent correlation with higher order lags. This is evidence that we should take a **first difference** of the data to remove this autocorrelation.\n",
    "\n",
    "This makes sense, since we are trying to capture the effect of recent lags in our ARMA models, and with high correlation between distant lags, our models will not come close to the true process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shaded area of the graph is the convidence interval.  When the correlation drops into the shaded area, that means there is no longer statistically significant correlation between lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(train.diff().dropna());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(train.diff().dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This autocorrelation plot can now be used to get an idea of a potential MA term.  Our differenced series shows negative significant correlation at lag of 1 suggests adding 1 MA term.  There is also a statistically significant 2nd, term, so adding another MA is another possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> If the ACF of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative--i.e., if the series appears slightly \"overdifferenced\"--then consider adding an MA term to the model. The lag at which the ACF cuts off is the indicated number of MA terms. [Duke](https://people.duke.edu/~rnau/411arim3.htm#signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above suggest that we should try a 1st order differenced MA(1) or MA(2) model on our weekly gun offense data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF can be used to identify the possible structure of time series data. That can be tricky going forward as there often isn’t a single clear-cut interpretation of a sample autocorrelation function.\n",
    "\n",
    "Luckily, we have auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='auto_arima'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us, we have a Python package that will help us determine optimal terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "auto_arima(train, start_p=0, start_q=0, max_p=6, max_q=3, seasonal=False, trace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to auto_arima, our optimal model is a first order differenced, AR(1)MA(2) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our training predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_model = ARIMA(train, (1,1,2)).fit()\n",
    "y_hat_train = aa_model.predict(typ='levels')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_hat_train)\n",
    "ax.plot(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom in:\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_hat_train[50:70])\n",
    "ax.plot(train[50:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(train[1:], y_hat_train))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chosen our parameters, let's try our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = aa_model.predict(start=test.index[0], end=test.index[-1],typ='levels')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_hat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_hat_test)\n",
    "ax.plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions on the test set certainly leave something to be desired.  \n",
    "\n",
    "Let's take another look at our autocorrelation function of the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(ts_weekly);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(ts_weekly, lags=75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a wave of correlation at around 50 lags.\n",
    "What is going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![verkempt](https://media.giphy.com/media/l3vRhBz4wCpJ9aEuY/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we may have some other forms of seasonality.  Luckily, we have SARIMA, which stands for Seasonal Auto Regressive Integrated Moving Average.  That is a lot.  The statsmodels package is actually called SARIMAX.  The X stands for exogenous, and we are only dealing with endogenous variables, but we can use SARIMAX as a SARIMA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seasonal ARIMA model is classified as an **ARIMA(p,d,q)x(P,D,Q)** model, \n",
    "\n",
    "    **p** = number of autoregressive (AR) terms \n",
    "    **d** = number of differences \n",
    "    **q** = number of moving average (MA) terms\n",
    "     \n",
    "    **P** = number of seasonal autoregressive (SAR) terms \n",
    "    **D** = number of seasonal differences \n",
    "    **Q** = number of seasonal moving average (SMA) terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "p = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, [1], q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 52) for x in list(itertools.product(p, [1], q))]\n",
    "print('Examples of parameter for SARIMA...')\n",
    "for i in pdq:\n",
    "    for s in seasonal_pdq:\n",
    "        print('SARIMAX: {} x {}'.format(i, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod =SARIMAX(train,order=param,seasonal_order=param_seasonal,enforce_stationarity=False,enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{} - AIC:{}'.format(param,param_seasonal,results.aic))\n",
    "        except: \n",
    "            print('hello')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the third from the bottom, ARIMA(1, 1, 1)x(0, 1, 1, 52)12 - AIC:973.5518935855749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sari_mod =SARIMAX(train,order=(1,1,1),\n",
    "                  seasonal_order=(0,1,1,52),\n",
    "                  enforce_stationarity=False,\n",
    "                  enforce_invertibility=False).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = sari_mod.predict(typ='levels')\n",
    "y_hat_test = sari_mod.predict(start=test.index[0], end=test.index[-1],typ='levels')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train)\n",
    "ax.plot(test)\n",
    "ax.plot(y_hat_train)\n",
    "ax.plot(y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom in on test\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(test)\n",
    "ax.plot(y_hat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's predict into the future.\n",
    "\n",
    "To do so, we refit to our entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sari_mod =SARIMAX(ts_weekly,order=(1,1,1),seasonal_order=(0,1,1,52),enforce_stationarity=False,enforce_invertibility=False).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = sari_mod.forecast(steps = 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ts_weekly)\n",
    "ax.plot(forecast)\n",
    "ax.set_title('Chicago Gun Crime Predictions\\n One Year out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearn",
   "language": "python",
   "name": "mlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
