{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82b34b4-827b-4dfd-a88b-3b85ee114c08",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034cce9-9f55-412f-89d6-21cf3af8e95b",
   "metadata": {
    "index": 1
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392e386-65d3-49fc-92da-5782767f1749",
   "metadata": {
    "index": 3
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eadac-721e-4a37-b098-f5fbfa5f62bb",
   "metadata": {
    "index": 5
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dcdc73-df54-43b2-80c7-07b8e7ff8fa6",
   "metadata": {
    "index": 7
   },
   "outputs": [],
   "source": [
    "print('Train Shape:', train_images.shape)\n",
    "print('Test Shape:', test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3a348-5157-48bc-ba1f-8031f373606b",
   "metadata": {
    "index": 9
   },
   "outputs": [],
   "source": [
    "# Convolutional Layers expect the data to \n",
    "# have four dimensions:\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a1b84-fda0-49b6-bcf1-0313c9b53256",
   "metadata": {
    "index": 11
   },
   "source": [
    "### Create a baseline model - No Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a6917-bae1-48f0-b579-c720e08f72d7",
   "metadata": {
    "index": 12
   },
   "outputs": [],
   "source": [
    "# Import a Sequential model\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Import Dense, Flatten and Input layers\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Define a function called `baseline_model`\n",
    "# with an input called `image`\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "    \n",
    "    # Create a list called `layers`.\n",
    "    # This list should contain all of the layers\n",
    "    # of the model.\n",
    "    #### The model should have the following architecture:\n",
    "    ####### 1. Input layer\n",
    "    ####### 2. Flatten layer\n",
    "    ####### 3. A dense layer\n",
    "    #######      - 100 units\n",
    "    #######      - Relu activation\n",
    "    ####### 4. A dense output layer\n",
    "    #######      - The number are labels = number of units\n",
    "    #######      - A softmax activation\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Create a Sequential model and \n",
    "    # add the layers\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # =================== \n",
    "        \n",
    "    # Compile model\n",
    "    #### Set loss to sparse categorical cross entropy\n",
    "    #### Set the optimizer to adam\n",
    "    #### Use accuracy for the metric\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # =================== \n",
    "    \n",
    "    # Return the model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91c5b1-0ccb-433d-8517-d5cac56d9f0a",
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc05e71-170f-49bd-b723-051c04f7745c",
   "metadata": {
    "index": 16
   },
   "outputs": [],
   "source": [
    "# Output a summary\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722dbc10-325b-463d-87f5-4ccf4d0948ed",
   "metadata": {
    "index": 18
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Use 20% of your training data for validation\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7d6a8-122a-4680-ac9b-07f8e3c3ff19",
   "metadata": {
    "index": 20
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a0bbd-9363-4fdc-bf4c-dbf3c852167e",
   "metadata": {
    "index": 21
   },
   "source": [
    "A convolutional neural network is a neural network with **convolutional layers**. CNNs are mainly used for image recognition/classification. They can be used for video analysis, NLP (sentiment analysis, topic modeling), and speech recognition. \n",
    "\n",
    "### How do our brains see an image? \n",
    "\n",
    "We might see some fluffy tail, a wet nose, flappy ears, and a good boy and conclude we are probably seeing a dog. There is not one singular thing about a dog that our brain recognizes as a dog but an amalgamation of different patterns that allow us to make a probable guess.  \n",
    "\n",
    "<img src='images/chihuahua.jpeg'/>\n",
    "\n",
    "### How do computers see images?\n",
    "\n",
    "<img src='images/architecture.jpeg' width=700/>\n",
    "\n",
    "To computers, color images are a 3D object - composed of 3 matrices - one for each primary color that can be combined in varying intensities to create different colors. Each element in a matrix represents the location of a pixel and contains a number between 0 and 255 which indicates the intensity of the corresponding primary color in that pixel.\n",
    "\n",
    "<img src='images/rgb.png'/>\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "**To *convolve* means to roll together**. CNNs make use of linear algebra to identify patterns using the pixel values (intensity of R,G, or B). By **taking a small matrix and moving it across an image and multiplying them together every time it moves**, our network can mathematically identify patterns in these images. This small matrix is known as a *kernel* or *filter* and each one is designed to identify a particular pattern in an image (edges, shapes, etc.)\n",
    "\n",
    "<img src='images/convolve.gif' width=500/>\n",
    "\n",
    "When a filter is \"rolled over\" an image, the resulting matrix is called a **feature map** - literally a map of where each pattern of feature is in the image. Elements with higher values indicate the presence of a pattern the filter is looking for. The values (or weights) of the filter are adjusted during back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9d78f-85a3-4205-b989-13dca11d79ae",
   "metadata": {
    "index": 22
   },
   "source": [
    "### Let's look at a simple example to illustrate what a convolution layer is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f0190-8f00-41c6-86d8-60cd32333e35",
   "metadata": {
    "index": 23
   },
   "source": [
    "$\\begin{bmatrix}\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0 \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "\\end{bmatrix}\\space*\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 \\\\\n",
    "    1 & 0 & -1 \\\\\n",
    "    1 & 0 & -1 \\\\ \n",
    "\\end{bmatrix}=\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Check out **[this excellent tool](https://deeplizard.com/resource/pavq7noze2)** to explore how filters isolate different features of an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866ca08-2035-4436-b4e8-02d772c9c4b4",
   "metadata": {
    "index": 24
   },
   "source": [
    "#### Convolutional layer parameters\n",
    "\n",
    "1. Padding: sometimes it is convenient to pad the input volume with zeros around the border. Helps with detecting patterns at the edge of an image\n",
    "1. Stride: the number of pixels to shift the filter on each \"roll\". The larger the stride, the smaller the feature map will be - but we will lose more information\n",
    "1. Kernel Regularization\n",
    "    - This represented the regularization for the weights inside the convolutional filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a47b5-204a-49d4-8c5b-e073a06d3dbf",
   "metadata": {
    "index": 25
   },
   "source": [
    "## Create a basic Convolutional Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ae090-df32-4e6c-a332-9a9dda36b1ae",
   "metadata": {
    "index": 26
   },
   "outputs": [],
   "source": [
    "# Import A 2-d Convolutional layer and 2-d Maxpooling \n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Define a function to build your model\n",
    "# That receives a single image as an input\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "    # Create a list of layers\n",
    "    ##### Input layer\n",
    "    ##### Convolutional layer\n",
    "    ########## - 32 filters\n",
    "    ########## - Filter size with width and height of 3\n",
    "    ########## - Activation: Your choice!\n",
    "    ##### A flattening layer\n",
    "    ##### A dense layer\n",
    "    ########## - Units: 128\n",
    "    ########## - Activation: Your choice!\n",
    "    ##### A dense layer\n",
    "    ########## - Units: The number of target classes\n",
    "    ########## - Activation: Softmax\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Add the layers to a Sequential model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Compile the model\n",
    "    ##### Loss: Sparse Categorical Crossentropy\n",
    "    ##### Optimizer: Adam\n",
    "    ##### Metric: Accuracy\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Return the model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7726b4-c29f-4400-90f3-667f17151e9a",
   "metadata": {
    "index": 28
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cf2d3-3227-448e-b530-8bd62ab8baaa",
   "metadata": {
    "index": 30
   },
   "outputs": [],
   "source": [
    "# Output a summary\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978c4c9-bb45-4839-a1d9-1c5d63c75909",
   "metadata": {
    "index": 32
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Use 20% of train as validation\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65740d45-92af-42fc-bbcd-a4b32e261aea",
   "metadata": {
    "index": 34
   },
   "source": [
    "# Some Classic Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aeaab8-654a-4e54-a93f-7aa0902643d7",
   "metadata": {
    "index": 35
   },
   "source": [
    "## LeNet - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a803f-086c-45c0-aed6-6f5d369ea212",
   "metadata": {
    "index": 36
   },
   "outputs": [],
   "source": [
    "# Import a 2-D average pooling layer\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Define function called \n",
    "# `LeNet5` that receives a single image\n",
    "# as an input\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "    layers = [\n",
    "        # Input layer\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Convolution layer \n",
    "        ##### Filters: 6\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Average Pooling\n",
    "        ##### Side: 2\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Convolution layer\n",
    "        ##### Filters: 16\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Average Pooling layer\n",
    "        ##### Size: 2\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Flatten layer\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Dense layer\n",
    "        ##### Number of units: 120\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Dense layer\n",
    "        ##### Number of units: 82\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Dense layer\n",
    "        ##### Number of units: Number of class labels\n",
    "        ##### Activation: softmax\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        ]\n",
    "    \n",
    "    # Add layers to a sequential model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Compile model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Return model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95887e4-32f7-486d-a90a-f8dc9bc77da2",
   "metadata": {
    "index": 37
   },
   "outputs": [],
   "source": [
    "# Import a 2-D average pooling layer\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "\n",
    "# Define function called \n",
    "# `LeNet5` that receives a single image\n",
    "# as an input\n",
    "def LeNet5(image):\n",
    "    \n",
    "    layers = [\n",
    "        # Input layer\n",
    "        Input(image.shape),\n",
    "        # Convolution layer \n",
    "        ##### Filters: 6\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        Conv2D(6, (5, 5), activation='relu'),\n",
    "        # Average Pooling\n",
    "        ##### Side: 2\n",
    "        AveragePooling2D((2,2)),\n",
    "        # Convolution layer\n",
    "        ##### Filters: 16\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        Conv2D(16, (5,5), activation='relu'),\n",
    "        # Average Pooling layer\n",
    "        ##### Size: 2\n",
    "        AveragePooling2D((2,2)),\n",
    "        # Flatten layer\n",
    "        Flatten(),\n",
    "        # Dense layer\n",
    "        ##### Number of units: 120\n",
    "        ##### Activation: relu\n",
    "        Dense(120, activation='relu'),\n",
    "        # Dense layer\n",
    "        ##### Number of units: 82\n",
    "        ##### Activation: relu\n",
    "        Dense(82, activation='relu'),\n",
    "        # Dense layer\n",
    "        ##### Number of units: Number of class labels\n",
    "        ##### Activation: softmax\n",
    "        Dense(10, activation='softmax'),\n",
    "        ]\n",
    "    \n",
    "    # Add layers to a sequential model\n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29064df-0a4e-4df2-9b0b-b61a7554a164",
   "metadata": {
    "index": 38
   },
   "outputs": [],
   "source": [
    "model_3 = LeNet5(train_images[0])\n",
    "model_3.fit(train_images, train_labels, validation_split=.2, epochs=10, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcec20d-84b3-4014-a395-72960e4bbe91",
   "metadata": {
    "index": 39
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "def AlexNet(image):\n",
    "    \n",
    "    layers = [\n",
    "        Input(image.shape),\n",
    "        Conv2D(96, (11,11), activation = 'relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(384, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(9216, activation='relu'),\n",
    "        Dropout(.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b29a1-6396-4243-9360-5a54cd3e81a7",
   "metadata": {
    "index": 40
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_images(images, pad=10):\n",
    "\n",
    "    return np.pad(images,((0,0),(pad,pad),(pad,pad),(0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fb8da-e899-4bf1-82c6-5a67d65e1f5d",
   "metadata": {
    "index": 41
   },
   "outputs": [],
   "source": [
    "padded_images = pad_images(train_images, pad=100)\n",
    "model_4 = AlexNet(padded_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b92476-074b-4feb-8051-711a09812266",
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "model_4.fit(padded_images, train_labels, validation_split=.2, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57ac95-9833-4267-8f77-6d87ed51404d",
   "metadata": {
    "index": 43
   },
   "outputs": [],
   "source": [
    "def VGG16(image):\n",
    "    \n",
    "    layers = [\n",
    "        Input(image.shape),\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(100, activation='softmax')\n",
    "    ]\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73d2a6-8257-48f2-9221-0d19045b5507",
   "metadata": {
    "index": 44
   },
   "outputs": [],
   "source": [
    "model_5 = VGG16(padded_images[0])\n",
    "model_5.fit(padded_images, train_labels, validation_split=.2, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd41f7-8e74-4ea6-bde6-45d4911ff447",
   "metadata": {
    "index": 45
   },
   "source": [
    "This model is so frequently used that a stock `VGG16` model is available as a **transfer learning** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997ccc9-334d-4f13-90c0-d5f8d63e22f0",
   "metadata": {
    "index": 46
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "model_6 = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dbd55-8884-45fd-ae90-576489d7143c",
   "metadata": {
    "index": 47
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Add, BatchNormalization, Activation\n",
    "\n",
    "def SimpleResNet(image):\n",
    "\n",
    "    input_layer = Input(image.shape)\n",
    "   \n",
    "    X = Conv2D(64, (3,3), activation='relu')(input_layer)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut] )# Skip Connection\n",
    "    X = Activation('relu')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=X, name='ResNet')\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f79bf5-8ad0-4e9f-a6aa-69a28cc98fb8",
   "metadata": {
    "index": 48
   },
   "outputs": [],
   "source": [
    "model_7 = SimpleResNet(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19adac9-ed43-4f50-bf3a-f4032bd3ad06",
   "metadata": {
    "index": 49
   },
   "outputs": [],
   "source": [
    "model_7.fit(train_images, train_labels, validation_split=.2, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d8075-8e60-497d-a3ef-90c08c99b5d5",
   "metadata": {
    "index": 50
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3c5fb-bcd0-4576-b1e9-8d473f7cf1bd",
   "metadata": {
    "index": 51
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
    "\n",
    "\n",
    "def CustomResNet50(image):\n",
    "    \n",
    "    \n",
    "    \n",
    "    base_model = ResNet50(include_top=False,\n",
    "                          weights=None,\n",
    "                          input_shape=image.shape)\n",
    "    \n",
    "    X = base_model.output\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "\n",
    "    \n",
    "    model = Model(inputs = base_model.input, outputs=X)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002699f-c400-4de3-b47c-6ea75568aeb3",
   "metadata": {
    "index": 52
   },
   "outputs": [],
   "source": [
    "resnet_images = pad_images(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bbe08-9c63-4c66-b3c8-75f3227a5bdd",
   "metadata": {
    "index": 53
   },
   "outputs": [],
   "source": [
    "model_8 = CustomResNet50(resnet_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eec8a1-3dba-4800-a071-22e8a9bd6d74",
   "metadata": {
    "index": 54
   },
   "outputs": [],
   "source": [
    "model_8.fit(resnet_images, train_labels, validation_split=.2, epochs=10, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
