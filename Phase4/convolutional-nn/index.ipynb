{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "index": 1
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "index": 3
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "index": 5
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "index": 7
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (60000, 28, 28)\n",
      "Test Shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('Train Shape:', train_images.shape)\n",
    "print('Test Shape:', test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 9
   },
   "outputs": [],
   "source": [
    "# Convolutional Layers expect the data to \n",
    "# have four dimensions:\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 11
   },
   "source": [
    "### Create a baseline model - No Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 12
   },
   "outputs": [],
   "source": [
    "# Import a Sequential model\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Import Dense, Flatten and Input layers\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Define a function called `baseline_model`\n",
    "# with an input called `image`\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "    \n",
    "    # Create a list called `layers`.\n",
    "    # This list should contain all of the layers\n",
    "    # of the model.\n",
    "    #### The model should have the following architecture:\n",
    "    ####### 1. Input layer\n",
    "    ####### 2. Flatten layer\n",
    "    ####### 3. A dense layer\n",
    "    #######      - 100 units\n",
    "    #######      - Relu activation\n",
    "    ####### 4. A dense output layer\n",
    "    #######      - The number are labels = number of units\n",
    "    #######      - A softmax activation\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Create a Sequential model and \n",
    "    # add the layers\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # =================== \n",
    "        \n",
    "    # Compile model\n",
    "    #### Set loss to sparse categorical cross entropy\n",
    "    #### Set the optimizer to adam\n",
    "    #### Use accuracy for the metric\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # =================== \n",
    "    \n",
    "    # Return the model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 16
   },
   "outputs": [],
   "source": [
    "# Output a summary\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 18
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Use 20% of your training data for validation\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 20
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 21
   },
   "source": [
    "A convolutional neural network is a neural network with **convolutional layers**. CNNs are mainly used for image recognition/classification. They can be used for video analysis, NLP (sentiment analysis, topic modeling), and speech recognition. \n",
    "\n",
    "### How do our brains see an image? \n",
    "\n",
    "We might see some fluffy tail, a wet nose, flappy ears, and a good boy and conclude we are probably seeing a dog. There is not one singular thing about a dog that our brain recognizes as a dog but an amalgamation of different patterns that allow us to make a probable guess.  \n",
    "\n",
    "<img src='images/chihuahua.jpeg'/>\n",
    "\n",
    "### How do computers see images?\n",
    "\n",
    "<img src='images/architecture.jpeg' width=700/>\n",
    "\n",
    "To computers, color images are a 3D object - composed of 3 matrices - one for each primary color that can be combined in varying intensities to create different colors. Each element in a matrix represents the location of a pixel and contains a number between 0 and 255 which indicates the intensity of the corresponding primary color in that pixel.\n",
    "\n",
    "<img src='images/rgb.png'/>\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "**To *convolve* means to roll together**. CNNs make use of linear algebra to identify patterns using the pixel values (intensity of R,G, or B). By **taking a small matrix and moving it across an image and multiplying them together every time it moves**, our network can mathematically identify patterns in these images. This small matrix is known as a *kernel* or *filter* and each one is designed to identify a particular pattern in an image (edges, shapes, etc.)\n",
    "\n",
    "<img src='images/convolve.gif' width=500/>\n",
    "\n",
    "When a filter is \"rolled over\" an image, the resulting matrix is called a **feature map** - literally a map of where each pattern of feature is in the image. Elements with higher values indicate the presence of a pattern the filter is looking for. The values (or weights) of the filter are adjusted during back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 22
   },
   "source": [
    "### Let's look at a simple example to illustrate what a convolution layer is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 23
   },
   "source": [
    "$\\begin{bmatrix}\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0 \\\\\n",
    "    10 & 10 & 10 & 0 & 0 & 0  \\\\\n",
    "\\end{bmatrix}\\space*\\\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 \\\\\n",
    "    1 & 0 & -1 \\\\\n",
    "    1 & 0 & -1 \\\\ \n",
    "\\end{bmatrix}=\\\n",
    "\\begin{bmatrix}\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "    0 & 30 & 30 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Check out **[this excellent tool](https://deeplizard.com/resource/pavq7noze2)** to explore how filters isolate different features of an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 24
   },
   "source": [
    "#### Convolutional layer parameters\n",
    "\n",
    "1. Padding: sometimes it is convenient to pad the input volume with zeros around the border. Helps with detecting patterns at the edge of an image\n",
    "1. Stride: the number of pixels to shift the filter on each \"roll\". The larger the stride, the smaller the feature map will be - but we will lose more information\n",
    "1. Kernel Regularization\n",
    "    - This represented the regularization for the weights inside the convolutional filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "## Create a basic Convolutional Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 26
   },
   "outputs": [],
   "source": [
    "# Import A 2-d Convolutional layer and 2-d Maxpooling \n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Define a function to build your model\n",
    "# That receives a single image as an input\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "    # Create a list of layers\n",
    "    ##### Input layer\n",
    "    ##### Convolutional layer\n",
    "    ########## - 32 filters\n",
    "    ########## - Filter size with width and height of 3\n",
    "    ########## - Activation: Your choice!\n",
    "    ##### A flattening layer\n",
    "    ##### A dense layer\n",
    "    ########## - Units: 128\n",
    "    ########## - Activation: Your choice!\n",
    "    ##### A dense layer\n",
    "    ########## - Units: The number of target classes\n",
    "    ########## - Activation: Softmax\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Add the layers to a Sequential model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Compile the model\n",
    "    ##### Loss: Sparse Categorical Crossentropy\n",
    "    ##### Optimizer: Adam\n",
    "    ##### Metric: Accuracy\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Return the model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 28
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 30
   },
   "outputs": [],
   "source": [
    "# Output a summary\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 32
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Use 20% of train as validation\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 34
   },
   "source": [
    "# Some Classic Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 35
   },
   "source": [
    "## LeNet - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 36
   },
   "outputs": [],
   "source": [
    "# Import a 2-D average pooling layer\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "# Define function called \n",
    "# `LeNet5` that receives a single image\n",
    "# as an input\n",
    "# ===================\n",
    "# YOUR CODE HERE\n",
    "# ===================\n",
    "\n",
    "    layers = [\n",
    "        # Input layer\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Convolution layer \n",
    "        ##### Filters: 6\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Average Pooling\n",
    "        ##### Side: 2\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Convolution layer\n",
    "        ##### Filters: 16\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Average Pooling layer\n",
    "        ##### Size: 2\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Flatten layer\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Dense layer\n",
    "        ##### Number of units: 120\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Dense layer\n",
    "        ##### Number of units: 82\n",
    "        ##### Activation: relu\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        # Dense layer\n",
    "        ##### Number of units: Number of class labels\n",
    "        ##### Activation: softmax\n",
    "        # ===================\n",
    "        # YOUR CODE HERE\n",
    "        # ===================\n",
    "        ]\n",
    "    \n",
    "    # Add layers to a sequential model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Compile model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ===================\n",
    "    \n",
    "    # Return model\n",
    "    # ===================\n",
    "    # YOUR CODE HERE\n",
    "    # ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 37
   },
   "outputs": [],
   "source": [
    "# Import a 2-D average pooling layer\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "\n",
    "# Define function called \n",
    "# `LeNet5` that receives a single image\n",
    "# as an input\n",
    "def LeNet5(image):\n",
    "    \n",
    "    layers = [\n",
    "        # Input layer\n",
    "        Input(image.shape),\n",
    "        # Convolution layer \n",
    "        ##### Filters: 6\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        Conv2D(6, (5, 5), activation='relu'),\n",
    "        # Average Pooling\n",
    "        ##### Side: 2\n",
    "        AveragePooling2D((2,2)),\n",
    "        # Convolution layer\n",
    "        ##### Filters: 16\n",
    "        ##### Filter size: 5\n",
    "        ##### Activation: relu\n",
    "        Conv2D(16, (5,5), activation='relu'),\n",
    "        # Average Pooling layer\n",
    "        ##### Size: 2\n",
    "        AveragePooling2D((2,2)),\n",
    "        # Flatten layer\n",
    "        Flatten(),\n",
    "        # Dense layer\n",
    "        ##### Number of units: 120\n",
    "        ##### Activation: relu\n",
    "        Dense(120, activation='relu'),\n",
    "        # Dense layer\n",
    "        ##### Number of units: 82\n",
    "        ##### Activation: relu\n",
    "        Dense(82, activation='relu'),\n",
    "        # Dense layer\n",
    "        ##### Number of units: Number of class labels\n",
    "        ##### Activation: softmax\n",
    "        Dense(10, activation='softmax'),\n",
    "        ]\n",
    "    \n",
    "    # Add layers to a sequential model\n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 38
   },
   "outputs": [],
   "source": [
    "model_3 = LeNet5(train_images[0])\n",
    "model_3.fit(train_images, train_labels, validation_split=.2, epochs=10, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 39
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "def AlexNet(image):\n",
    "    \n",
    "    layers = [\n",
    "        Input(image.shape),\n",
    "        Conv2D(96, (11,11), activation = 'relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(384, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(9216, activation='relu'),\n",
    "        Dropout(.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 40
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_images(images, pad=10):\n",
    "\n",
    "    return np.pad(images,((0,0),(pad,pad),(pad,pad),(0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 41
   },
   "outputs": [],
   "source": [
    "padded_images = pad_images(train_images, pad=100)\n",
    "model_4 = AlexNet(padded_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "model_4.fit(padded_images, train_labels, validation_split=.2, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 43
   },
   "outputs": [],
   "source": [
    "def VGG16(image):\n",
    "    \n",
    "    layers = [\n",
    "        Input(image.shape),\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(100, activation='softmax')\n",
    "    ]\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 44
   },
   "outputs": [],
   "source": [
    "model_5 = VGG16(padded_images[0])\n",
    "model_5.fit(padded_images, train_labels, validation_split=.2, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 45
   },
   "source": [
    "This model is so frequently used that a stock `VGG16` model is available as a **transfer learning** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 46
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "model_6 = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 47
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Add, BatchNormalization, Activation\n",
    "\n",
    "def SimpleResNet(image):\n",
    "\n",
    "    input_layer = Input(image.shape)\n",
    "   \n",
    "    X = Conv2D(64, (3,3), activation='relu')(input_layer)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut] )# Skip Connection\n",
    "    X = Activation('relu')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=X, name='ResNet')\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 48
   },
   "outputs": [],
   "source": [
    "model_7 = SimpleResNet(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 49
   },
   "outputs": [],
   "source": [
    "model_7.fit(train_images, train_labels, validation_split=.2, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 50
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 51
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
    "\n",
    "\n",
    "def CustomResNet50(image):\n",
    "    \n",
    "    \n",
    "    \n",
    "    base_model = ResNet50(include_top=False,\n",
    "                          weights=None,\n",
    "                          input_shape=image.shape)\n",
    "    \n",
    "    X = base_model.output\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "\n",
    "    \n",
    "    model = Model(inputs = base_model.input, outputs=X)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 52
   },
   "outputs": [],
   "source": [
    "resnet_images = pad_images(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 53
   },
   "outputs": [],
   "source": [
    "model_8 = CustomResNet50(resnet_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 54
   },
   "outputs": [],
   "source": [
    "model_8.fit(resnet_images, train_labels, validation_split=.2, epochs=10, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
